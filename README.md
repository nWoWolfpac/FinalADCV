# BigEarthNet â†’ DFC2020 Segmentation Pipeline

Pipeline huáº¥n luyá»‡n mÃ´ hÃ¬nh UNet cho bÃ i toÃ¡n semantic segmentation trÃªn dataset DFC2020, sá»­ dá»¥ng encoder pretrained trÃªn BigEarthNet.

## ğŸ“‹ Má»¥c Lá»¥c

- [Tá»•ng Quan](#tá»•ng-quan)
- [CÃ i Äáº·t](#cÃ i-Ä‘áº·t)
- [Cáº¥u TrÃºc Project](#cáº¥u-trÃºc-project)
- [HÆ°á»›ng Dáº«n Training](#hÆ°á»›ng-dáº«n-training)
- [HÆ°á»›ng Dáº«n Evaluation](#hÆ°á»›ng-dáº«n-evaluation)
- [Cáº¥u HÃ¬nh](#cáº¥u-hÃ¬nh)
- [Káº¿t Quáº£](#káº¿t-quáº£)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Tá»•ng Quan

### MÃ´ Táº£

Project nÃ y implement má»™t pipeline 2-stage Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh UNet cho semantic segmentation trÃªn dataset DFC2020:

1. **Stage 1**: Pretrain encoder trÃªn BigEarthNet (19 classes classification)
2. **Stage 2**: Fine-tune UNet trÃªn DFC2020 (8 classes segmentation)

### Dataset

- **DFC2020**: Semantic segmentation dataset vá»›i 8 classes
  - Input: 12 channels (2 radar + 10 optical tá»« Sentinel-1 vÃ  Sentinel-2)
  - Output: Segmentation mask vá»›i 8 classes
  - Repository: `GFM-Bench/DFC2020` trÃªn HuggingFace

### Model Architecture

- **Encoder**: ResNet50/ResNet101 pretrained trÃªn BigEarthNet
- **Decoder**: UNet vá»›i skip connections
- **Input**: 12-channel images (96Ã—96)
- **Output**: 8-class segmentation masks

---

## ğŸš€ CÃ i Äáº·t

### YÃªu Cáº§u Há»‡ Thá»‘ng

- Python >= 3.8
- CUDA-capable GPU (khuyáº¿n nghá»‹ 8GB+ VRAM)
- 16GB+ RAM

### CÃ i Äáº·t Dependencies

```bash
# Clone repository
!git clone --branch Unet --single-branch https://github.com/nWoWolfpac/FinalADCV
cd FinalADCV

# CÃ i Ä‘áº·t packages
pip install -r requirements.txt
```

### CÃ i Äáº·t HuggingFace Dataset

Dataset sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c táº£i vá» tá»« HuggingFace khi cháº¡y training láº§n Ä‘áº§u. Äáº£m báº£o báº¡n Ä‘Ã£ Ä‘Äƒng nháº­p HuggingFace:

```bash
huggingface-cli login
```

---

## ğŸ“ Cáº¥u TrÃºc Project

```
FinalADCV/
â”œâ”€â”€ config.py                 # Cáº¥u hÃ¬nh chÃ­nh (hyperparameters, paths)
â”œâ”€â”€ training_unet.py          # Script training UNet
â”œâ”€â”€ evaluate_unet.py          # Script evaluation vÃ  visualization
â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py       # Encoder pretrained trÃªn BigEarthNet
â”‚   â”‚   â””â”€â”€ unet.py          # UNet architecture
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ dataset_utils.py # Dataset loading vÃ  preprocessing
â”‚   â””â”€â”€ utils.py             # Trainer, metrics, visualization
â”‚
â”œâ”€â”€ checkpoints/             # Model checkpoints (tá»± Ä‘á»™ng táº¡o)
â”œâ”€â”€ logs/                    # Training logs (tá»± Ä‘á»™ng táº¡o)
â”œâ”€â”€ visualizations/          # Prediction visualizations (tá»± Ä‘á»™ng táº¡o)
â”‚
â”œâ”€â”€ land-cover-segmentationsegmentation.ipynb       # Notebook Ä‘á»ƒ explore dataset
â”œâ”€â”€ test_input_sizes.py      # Script test vá»›i cÃ¡c input_size khÃ¡c nhau
â””â”€â”€ README.md               # File nÃ y
```

---

## ğŸ‹ï¸ HÆ°á»›ng Dáº«n Training

### Training CÆ¡ Báº£n

```bash
# Training vá»›i ResNet50 encoder (máº·c Ä‘á»‹nh)
python training_unet.py --backbone resnet50

# Training vá»›i ResNet101 encoder
python training_unet.py --backbone resnet101

# Training vá»›i dropout rate tÃ¹y chá»‰nh
python training_unet.py --backbone resnet50 --dropout 0.2
```

### CÃ¡c Tham Sá»‘ Training

| Tham sá»‘ | MÃ´ táº£ | Máº·c Ä‘á»‹nh |
|---------|-------|----------|
| `--backbone` | Encoder backbone (resnet50, resnet101) | resnet50 |
| `--dropout` | Dropout rate cho decoder | 0.1 |
| `--resume` | ÄÆ°á»ng dáº«n checkpoint Ä‘á»ƒ resume training | None |

### Thay Äá»•i Input Size

Äá»ƒ training vá»›i input size khÃ¡c (128Ã—128 hoáº·c 256Ã—256), sá»­a trong `config.py`:

```python
STAGE2 = {
    "input_size": 128,        # Thay Ä‘á»•i tá»« 96
    "batch_size": 12,          # Giáº£m batch_size tÆ°Æ¡ng á»©ng
    # ... cÃ¡c config khÃ¡c
}
```

**LÆ°u Ã½ vá» Batch Size:**
- `input_size=96`: `batch_size=16` (GPU 8GB+)
- `input_size=128`: `batch_size=12` (GPU 8GB+)
- `input_size=256`: `batch_size=8` (GPU 16GB+)

### Training Process

1. **Load Dataset**: Tá»± Ä‘á»™ng táº£i tá»« HuggingFace
2. **Load Encoder**: Load pretrained encoder tá»« BigEarthNet
3. **Freeze Encoder**: Freeze encoder trong 5 epochs Ä‘áº§u
4. **Fine-tune**: Unfreeze vÃ  fine-tune toÃ n bá»™ model
5. **Save Checkpoints**: LÆ°u best model dá»±a trÃªn validation loss

### Output Files

Sau khi training, cÃ¡c file sau sáº½ Ä‘Æ°á»£c táº¡o:

```
checkpoints_unet/
â”œâ”€â”€ best_model.pth           # Best model checkpoint
â”œâ”€â”€ checkpoint_epoch_XX.pth  # Checkpoints theo epoch
â””â”€â”€ train_history.csv        # Training history (loss, metrics)
```

---

## ğŸ“Š HÆ°á»›ng Dáº«n Evaluation

### Evaluation trÃªn Test Set

```bash
# Evaluate vá»›i best model
python evaluate_unet.py \
    --checkpoint checkpoints_unet/best_model.pth \
    --backbone resnet50

# Evaluate vÃ  visualize predictions
python evaluate_unet.py \
    --checkpoint checkpoints_unet/best_model.pth \
    --backbone resnet50 \
    --visualize \
    --max_samples 10
```

### Metrics ÄÆ°á»£c TÃ­nh

- **Pixel Accuracy**: Tá»· lá»‡ pixels Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng
- **Mean IoU (mIoU)**: Intersection over Union trung bÃ¬nh
- **Per-class IoU**: IoU cho tá»«ng class

### Visualization

Script sáº½ tá»± Ä‘á»™ng táº¡o visualizations trong thÆ° má»¥c `visualizations/`:

- RGB composite tá»« Sentinel-2
- NDVI (Normalized Difference Vegetation Index)
- Radar composite (VV, VH)
- Ground Truth mask
- Predicted mask

---

## âš™ï¸ Cáº¥u HÃ¬nh

### File `config.py`

File cáº¥u hÃ¬nh chÃ­nh chá»©a táº¥t cáº£ hyperparameters:

```python
# Stage2: Segmentation on DFC2020
STAGE2 = {
    "input_size": 96,               # Input resolution
    "batch_size": 16,               # Batch size
    "num_epochs": 50,                # Sá»‘ epochs
    "freeze_encoder_epochs": 5,      # Sá»‘ epochs freeze encoder
    "encoder_lr": 1e-4,              # Learning rate cho encoder
    "decoder_lr": 5e-5,              # Learning rate cho decoder
    "weight_decay": 1e-4,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "mixed_precision": True,         # Sá»­ dá»¥ng mixed precision training
    "gradient_accumulation_steps": 1,
    # ...
}
```

### Normalization Values

Dataset Ä‘Æ°á»£c normalize vá»›i cÃ¡c giÃ¡ trá»‹ Ä‘Ã£ tÃ­nh tá»« training set:

```python
SENTINEL1_MEAN = [-12.190531, -19.398623]
SENTINEL1_STD = [5.172539, 6.659642]

SENTINEL2_MEAN = [995.894858, 901.027080, ...]  # 10 channels
SENTINEL2_STD = [257.763536, 299.047602, ...]   # 10 channels
```

---

## ğŸ“ˆ Káº¿t Quáº£

### Training Metrics

Training history Ä‘Æ°á»£c lÆ°u trong `train_history.csv` vá»›i cÃ¡c cá»™t:

- `epoch`: Sá»‘ epoch
- `train_loss`: Training loss
- `val_loss`: Validation loss
- `pixel_accuracy`: Pixel accuracy trÃªn validation set
- `mean_iou`: Mean IoU trÃªn validation set


## ğŸ“ VÃ­ Dá»¥ Sá»­ Dá»¥ng

### 1. Training tá»« Ä‘áº§u

```bash
# Step 1: Training vá»›i ResNet50
python training_unet.py --backbone resnet50

# Step 2: Sau khi training xong, evaluate
python evaluate_unet.py \
    --checkpoint checkpoints_unet/best_model.pth \
    --backbone resnet50 \
    --visualize
```

### 2. Resume Training

```bash
python training_unet.py \
    --backbone resnet50 \
    --resume checkpoints_unet/checkpoint_epoch_25.pth
```

### 3. Test vá»›i Input Size KhÃ¡c

```bash
# Sá»­a config.py: input_size = 128, batch_size = 12
python training_unet.py --backbone resnet50
```

### 4. File cháº¡y notebook

```bash
# Má»Ÿ Jupyter notebook
jupyter notebook land-cover-segmentation.ipynb
```

---
